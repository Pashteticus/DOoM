# Инструкция по хостингу LLM с VLLM и использованию API

Это руководство объясняет, как запустить сервер VLLM для хостинга вашей языковой модели и как отправлять к нему запросы по API

## 0. Рекомендация - использование `uv`

Для управления зависимостями и виртуальными окружениями Python рекомендуется использовать `uv` - быстрый установщик пакетов и менеджер виртуальных окружений.

**Установка `uv`:**

Инструкции по установке `uv` можно найти на [официальном сайте](https://github.com/astral-sh/uv). Обычно это делается с помощью `pip` или менеджера пакетов вашей ОС.

**Создание виртуального окружения (рекомендуется):**

```bash
# Создать окружение в папке .venv
uv venv venv -p 3.13
# Активировать окружение (Windows - PowerShell)
.venv\Scripts\activate
# Активировать окружение (Linux/macOS)
source .venv/bin/activate
```

Все последующие команды `uv pip install` следует выполнять в активированном виртуальном окружении.

## 1. Установка VLLM

Убедитесь, что у вас установлен Python (рекомендуется версия 3.8 или выше). Установите VLLM с помощью `uv`:

```bash
# Установка в активное виртуальное окружение
uv pip install vllm
```

Для поддержки CUDA 11.8:
```bash
uv pip install vllm --extra-index-url https://download.pytorch.org/whl/cu118
```
Замените `cu118` на вашу версию CUDA, если необходимо (например, `cu121`).

## 2. Запуск сервера VLLM

VLLM предоставляет OpenAI-совместимый сервер. Для его запуска используйте следующую команду в терминале:

```bash
python -m vllm.entrypoints.openai.api_server \
    --model <model_name_or_path> \
    --host <host_address> \
    --port <port_number> \
    --tensor-parallel-size <num_gpus> \
    # Другие необязательные параметры...
```

**Параметры:**

*   `<model_name_or_path>`: Имя модели с Hugging Face Hub (например, `mistralai/Mistral-7B-Instruct-v0.1`) или путь к локально скачанной модели. **Важно:** Это имя или путь должно точно совпадать с `model_name` в конфигурации бенчмарка (`configs/run.yaml`).
*   `<host_address>`: IP-адрес, на котором будет слушать сервер (например, `0.0.0.0` для доступа со всех интерфейсов или `127.0.0.1` для локального доступа). По умолчанию `localhost`.
*   `<port_number>`: Порт, на котором будет работать сервер (например, `8000`). По умолчанию `8000`.
*   `<num_gpus>`: Количество GPU, которое будет использоваться для запуска. По умолчанию `1`.

**Примечание:** Для больших моделей может потребоваться значительный объем видеопамяти (VRAM) на GPU.

**Пример запуска:**

```bash
python -m vllm.entrypoints.openai.api_server \
    --model mistralai/Mistral-7B-Instruct-v0.1 \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1
```

После запуска сервер будет готов принимать запросы по адресу `http://<host_address>:<port_number>`.

## 3. Использование с бенчмарком DeathMath

Этот бенчмарк использует конфигурационный файл (`configs/run.yaml`) для определения моделей и их API-эндпоинтов. Чтобы добавить вашу локально запущенную модель через VLLM в бенчмарк:

1.  **Запустите VLLM сервер**, как описано в разделе 2. Убедитесь, что вы используете адрес, доступный для бенчмарка (например, `localhost` или `127.0.0.1`, если бенчмарк запускается на той же машине).

2.  **Отредактируйте файл `configs/run.yaml`:**
    *   Добавьте имя вашей модели в список `model_list`.
    *   Добавьте новый блок конфигурации для вашей модели. Он должен выглядеть примерно так:

    ```yaml
    # ... другие модели ...

    my-local-model: # Замените на уникальное имя для вашей модели в конфиге
        model_name: <model_name_or_path> # ВАЖНО: Должно совпадать с параметром --model при запуске VLLM сервера
        endpoints:
            - api_base: "http://<host_address>:<port_number>/v1" # Адрес вашего VLLM сервера (например, "http://localhost:8000/v1")
              # api_key: "dummy-key" # API ключ обычно не требуется для локального VLLM.
        api_type: openai # Указывает, что используется OpenAI-совместимый API
        parallel: 1 # Количество параллельных запросов (настройте по необходимости)
        system_prompt: "Вы - полезный помощник по математике и физике. Ответьте на русском языке." # Или другой системный промпт
        max_tokens: 32000 # Максимальное количество токенов
        # request_delay: 0.1 # Задержка между запросами, если нужна
    ```

3.  **Запустите бенчмарк:**

    ```bash
    # Убедитесь, что ваше виртуальное окружение активировано
    # source .venv/bin/activate или .venv\Scripts\Activate.ps1

    # Запуск оценки для всех моделей в конфиге
    python runner.py

    # Или только для вашей модели (если вы хотите протестировать только ее)
    # Сначала измените model_list в run.yaml, оставив только my-local-model
    # python runner.py

    # Или для конкретного датасета (например, RussianMath)
    python runner.py --dataset russianmath
    ```

    Скрипт `runner.py` прочитает конфигурацию, найдет вашу локальную модель и будет отправлять ей запросы через указанный `api_base`.

## 4. Получение результатов

После завершения оценки скрипт `runner.py` обновит (или создаст) файл `results/leaderboard.md` с таблицей лидеров, включающей вашу модель. Также подробные результаты по каждой задаче для вашей модели можно найти в директории `results/details/my-local-model/` (замените `my-local-model` на имя вашей модели из `run.yaml`)